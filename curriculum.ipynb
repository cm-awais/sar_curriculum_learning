{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hard Passenger 29 8\n",
      "Hard Dredging 29 27\n",
      "Easy Fishing 466 319\n",
      "Moderate Fishing 72 247\n",
      "Hard Fishing 29 218\n",
      "Easy Cargo 466 1227\n",
      "Moderate Cargo 72 1155\n",
      "Hard Cargo 29 1126\n",
      "Hard GeneralCargo 29 12\n",
      "Hard Container 29 36\n",
      "Hard Tug 29 25\n",
      "Moderate Bulk 72 201\n",
      "Hard Bulk 29 172\n",
      "Moderate Tanker 72 76\n",
      "Hard Tanker 29 47\n",
      "Curriculum datasets created and saved in c_fusar_ready_bal/\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "from collections import defaultdict\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import random\n",
    "\n",
    "# Define class-wise sample requirements\n",
    "# fusar_subset_definitions = {\n",
    "#     \"easy\": {\"Cargo\": 437, \"Fishing\": 117},\n",
    "#     \"moderate\": {\"Cargo\": 314, \"Fishing\": 115, \"Bulk\": 81, \"Tanker\": 74},\n",
    "#     \"hard\": {\"Cargo\": 109, \"Fishing\": 58, \"Bulk\": 54, \"Tanker\": 44, \"Container\": 45, \"Dredging\": 39, \"Tug\": 43, \"GeneralCargo\": 32, \"Passenger\": 29},\n",
    "#     # \"test\": {\"Cargo\": 150, \"Fishing\": 58, \"Bulk\": 28, \"Tanker\": 18, \"Container\": 12, \"Dredging\": 10, \"Tug\": 8, \"GeneralCargo\": 7, \"Passenger\": 6},\n",
    "# }\n",
    "\n",
    "fusar_subset_definitions = {\n",
    "        'easy': {'Cargo': 408, 'Fishing': 314}, \n",
    "        'moderate': {'Bulk': 80, 'Tanker': 74, 'Cargo': 170, 'Fishing': 138}, \n",
    "        'hard': {'Container': 52, 'Dredging': 44, 'Tug': 43, 'GeneralCargo': 32, 'Passenger': 29, 'Bulk': 48, 'Tanker': 46, 'Cargo': 136, 'Fishing': 110}\n",
    "        }\n",
    "\n",
    "balanced_fusar_subset_definitions = {\n",
    "        'Easy': {'Cargo': 466, 'Fishing': 466}, \n",
    "        'Moderate': {'Tanker': 72, 'Bulk': 72, 'Cargo': 72, 'Fishing': 72}, \n",
    "        'Hard': {'Cargo': 29, 'Fishing': 29, 'Tanker': 29, 'Bulk': 29, 'Passenger': 29, 'Container': 29, 'Dredging': 29, 'Tug': 29, 'GeneralCargo': 29}}\n",
    "\n",
    "\n",
    "# Initialize paths\n",
    "fusar_data_dir = \"c_fusar/\"\n",
    "fusar_output_dir = \"c_fusar_ready_bal/\"\n",
    "os.makedirs(fusar_output_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "# Create output directories for subsets\n",
    "for subset in [\"easy\", \"moderate\", \"hard\", \"test\", \"validation\"]:\n",
    "    os.makedirs(os.path.join(fusar_output_dir, subset), exist_ok=True)\n",
    "\n",
    "# Function to collect data by class\n",
    "def collect_data_by_class(data_dir):\n",
    "    data_by_class = defaultdict(list)\n",
    "    for class_name in os.listdir(data_dir):\n",
    "        class_dir = os.path.join(data_dir, class_name)\n",
    "        if os.path.isdir(class_dir):\n",
    "            for file_name in os.listdir(class_dir):\n",
    "                data_by_class[class_name].append(os.path.join(class_dir, file_name))\n",
    "    return data_by_class\n",
    "\n",
    "# Split data into subsets\n",
    "def create_curriculum_subsets(data_by_class, subset_definitions, output_dir):\n",
    "    for class_name, file_paths in data_by_class.items():\n",
    "        random.shuffle(file_paths)  # Shuffle files for randomness\n",
    "        remaining = file_paths\n",
    "\n",
    "        for subset, class_limits in subset_definitions.items():\n",
    "            if class_name in class_limits:\n",
    "                limit = class_limits[class_name]\n",
    "                selected, remaining = remaining[:limit], remaining[limit:]\n",
    "                print(subset, class_name, class_limits[class_name], len(remaining))\n",
    "                save_files(selected, os.path.join(output_dir, subset, class_name))\n",
    "\n",
    "        # Split remaining data into validation and test sets\n",
    "        if remaining:\n",
    "            validation_files, test_files = train_test_split(\n",
    "                remaining, test_size=0.5, random_state=42\n",
    "            )\n",
    "            save_files(validation_files, os.path.join(output_dir, \"validation\", class_name))\n",
    "            save_files(test_files, os.path.join(output_dir, \"test\", class_name))\n",
    "\n",
    "# Function to save files into respective directories\n",
    "def save_files(file_paths, dest_dir):\n",
    "    os.makedirs(dest_dir, exist_ok=True)\n",
    "    for file_path in file_paths:\n",
    "        shutil.copy(file_path, os.path.join(dest_dir, os.path.basename(file_path)))\n",
    "\n",
    "\n",
    "# FUSAR \n",
    "# Collect data\n",
    "data_by_class = collect_data_by_class(fusar_data_dir)\n",
    "\n",
    "# Create curriculum subsets\n",
    "create_curriculum_subsets(data_by_class, balanced_fusar_subset_definitions, fusar_output_dir)\n",
    "\n",
    "print(f\"Curriculum datasets created and saved in {fusar_output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'easy': {'Cargo': 437, 'Fishing': 274},\n",
       " 'moderate': {'Cargo': 327,\n",
       "  'Fishing': 175,\n",
       "  'Bulk': 81,\n",
       "  'Tanker': 59,\n",
       "  'Container': 26,\n",
       "  'Dredging': 22},\n",
       " 'hard': {'Cargo': 109,\n",
       "  'Fishing': 58,\n",
       "  'Bulk': 54,\n",
       "  'Tanker': 59,\n",
       "  'Container': 26,\n",
       "  'Dredging': 22,\n",
       "  'Tug': 43,\n",
       "  'GeneralCargo': 32,\n",
       "  'Passenger': 29}}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fusar_subset_definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataLoaders created successfully.\n"
     ]
    }
   ],
   "source": [
    "# Fusar Loaders\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "import os\n",
    "\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.Resize((64, 64)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "new_class_to_idx = {'Cargo': 0, 'Fishing': 1, 'Bluk': 2, 'Dredging': 3, 'Container': 4, 'Tanker': 5, 'GeneralCargo': 6, 'Passenger': 7, 'Tug': 8}\n",
    "\n",
    "# Load datasets\n",
    "curriculum_data_dir = \"c_fusar_ready/\"\n",
    "easy_dataset = ImageFolder(os.path.join(curriculum_data_dir, \"easy\"), transform=transform_train)\n",
    "moderate_dataset = ImageFolder(os.path.join(curriculum_data_dir, \"moderate\"), transform=transform_train)\n",
    "hard_dataset = ImageFolder(os.path.join(curriculum_data_dir, \"hard\"), transform=transform_train)\n",
    "validation_dataset = ImageFolder(os.path.join(curriculum_data_dir, \"validation\"), transform=transform_train)\n",
    "test_dataset = ImageFolder(os.path.join(curriculum_data_dir, \"test\"), transform=transform_train)\n",
    "\n",
    "easy_dataset.class_to_idx = new_class_to_idx\n",
    "moderate_dataset.class_to_idx = new_class_to_idx\n",
    "hard_dataset.class_to_idx = new_class_to_idx\n",
    "validation_dataset.class_to_idx = new_class_to_idx\n",
    "test_dataset.class_to_idx = new_class_to_idx\n",
    "\n",
    "# Create DataLoaders\n",
    "batch_size = 32\n",
    "easy_loader = DataLoader(easy_dataset, batch_size=batch_size, shuffle=True)\n",
    "moderate_loader = DataLoader(moderate_dataset, batch_size=batch_size, shuffle=True)\n",
    "hard_loader = DataLoader(hard_dataset, batch_size=batch_size, shuffle=True)\n",
    "validation_loader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(\"DataLoaders created successfully.\")\n",
    "\n",
    "train_loaders = [easy_loader, moderate_loader, hard_loader]\n",
    "test_loaders = [validation_loader, test_loader]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# easy_dataset.class_to_idx, moderate_dataset.class_to_idx, hard_dataset.class_to_idx\n",
    "\n",
    "# new_class_to_idx = {'Cargo': 0, 'Fishing': 1, 'Dredging': 2, 'Container': 3, 'Tanker': 4, 'GeneralCargo': 5, 'Passenger': 6, 'Tug': 7}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "from torchvision import models\n",
    "\n",
    "\n",
    "class VGGModel(nn.Module):\n",
    "  def __init__(self, pretrained=True):\n",
    "    super(VGGModel, self).__init__()\n",
    "    self.features = models.vgg16(pretrained=pretrained).features  # Use VGG16 features\n",
    "    \n",
    "    # for param in self.features.parameters():\n",
    "    #   param.requires_grad = False  # Freeze pre-trained layers\n",
    "    self.avgpool = nn.AdaptiveAvgPool2d((7, 7))  # Global Average Pooling\n",
    "    self.classifier = nn.Sequential(\n",
    "            nn.Linear(512 * 7 * 7, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, 3)  # 3 output classes\n",
    "        )\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.features(x)\n",
    "    # print(x.shape)\n",
    "    x = self.avgpool(x)\n",
    "    x = torch.flatten(x, 1)\n",
    "    # print(x.shape)\n",
    "    x = self.classifier(x)\n",
    "    return x\n",
    "\n",
    "train_loaders = [easy_loader, moderate_loader, hard_loader]\n",
    "test_loaders = [validation_loader, test_loader]\n",
    "\n",
    "# Training function\n",
    "def train_model(model, train_loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    return running_loss / len(train_loader)\n",
    "\n",
    "# Training with curriculum\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = VGGModel().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 5\n",
    "for level, train_loader in enumerate(train_loaders):\n",
    "    print(f\"Training on curriculum level {level + 1}\")\n",
    "    # train_loader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        loss = train_model(model, train_loader, optimizer, criterion, device)\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss:.4f}\")\n",
    "        \n",
    "        # if level == 2:\n",
    "        #     eval_loss = train_model(model, test_loaders[0], optimizer, criterion, device)\n",
    "        #     print(f\"Eval: Epoch [{epoch+1}/{num_epochs}], Loss: {loss:.4f}\")\n",
    "\n",
    "\n",
    "print(\"Training completed with curriculum learning.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision import models\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = torch.device('cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on curriculum level 1\n",
      "Level 1, Epoch [1/3] - Train Loss: 2.8831, Val Loss: 3.2306\n",
      "Level 1, Epoch [2/3] - Train Loss: 0.6823, Val Loss: 4.2193\n",
      "Level 1, Epoch [3/3] - Train Loss: 0.7062, Val Loss: 3.2478\n",
      "Training on curriculum level 2\n",
      "Level 2, Epoch [1/3] - Train Loss: 2.0191, Val Loss: 1.4786\n",
      "Level 2, Epoch [2/3] - Train Loss: 1.4514, Val Loss: 1.5151\n",
      "Level 2, Epoch [3/3] - Train Loss: 1.4239, Val Loss: 1.4384\n",
      "Training on curriculum level 3\n",
      "Level 3, Epoch [1/3] - Train Loss: 2.4059, Val Loss: 2.1194\n",
      "Level 3, Epoch [2/3] - Train Loss: 2.1456, Val Loss: 1.9714\n",
      "Level 3, Epoch [3/3] - Train Loss: 2.0836, Val Loss: 1.5697\n",
      "Training completed with curriculum learning.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, 3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 32, 3, padding=1)\n",
    "        self.fc1 = nn.Linear(32 * 8 * 8, 128)\n",
    "        self.fc2 = nn.Linear(128, 9)\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(f\"Input shape to conv1: {x.shape}\")\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = torch.max_pool2d(x, 2)\n",
    "        print(f\"Shape after conv1: {x.shape}\")\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        x = torch.max_pool2d(x, 2)\n",
    "        print(f\"Shape after conv2: {x.shape}\")\n",
    "        x = x.view(x.size(0), -1)  # Ensure the batch dimension is preserved\n",
    "        print(f\"Shape after flattening: {x.shape}\")\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        return self.fc2(x)\n",
    "\n",
    "class VGGModel(nn.Module):\n",
    "  def __init__(self, pretrained=True):\n",
    "    super(VGGModel, self).__init__()\n",
    "    self.features = models.vgg16(pretrained=pretrained).features  # Use VGG16 features\n",
    "    \n",
    "    # for param in self.features.parameters():\n",
    "    #   param.requires_grad = False  # Freeze pre-trained layers\n",
    "    self.avgpool = nn.AdaptiveAvgPool2d((7, 7))  # Global Average Pooling\n",
    "    self.classifier = nn.Sequential(\n",
    "            nn.Linear(512 * 7 * 7, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, 9)  # 3 output classes\n",
    "        )\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.features(x)\n",
    "    # print(x.shape)\n",
    "    x = self.avgpool(x)\n",
    "    x = torch.flatten(x, 1)\n",
    "    # print(x.shape)\n",
    "    x = self.classifier(x)\n",
    "    return x\n",
    "\n",
    "# Training function\n",
    "def train_model(model, train_loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        # print(type(inputs))\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    return running_loss / len(train_loader)\n",
    "\n",
    "\n",
    "def validate_model(model, val_loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            # Compute loss\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item()\n",
    "    return running_loss / len(val_loader)\n",
    "\n",
    "\n",
    "# Training with curriculum\n",
    "model = VGGModel().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 3\n",
    "best_val_loss = float('inf')\n",
    "early_stop_patience = 9\n",
    "no_improve_epochs = 0\n",
    "\n",
    "for level, train_loader in enumerate(train_loaders):\n",
    "    print(f\"Training on curriculum level {level + 1}\")\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = train_model(model, train_loader, optimizer, criterion, device)\n",
    "        \n",
    "        # if level == 2:\n",
    "        val_loss = validate_model(model, test_loaders[0], criterion, device)\n",
    "        \n",
    "        print(f\"Level {level+1}, Epoch [{epoch+1}/{num_epochs}] - Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "        \n",
    "        # Early stopping logic\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            no_improve_epochs = 0\n",
    "        else:\n",
    "            no_improve_epochs += 1\n",
    "            if no_improve_epochs >= early_stop_patience:\n",
    "                print(f\"Early stopping at Level {level+1}, Epoch {epoch+1}\")\n",
    "                break\n",
    "\n",
    "print(\"Training completed with curriculum learning.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'pip' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "! pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
